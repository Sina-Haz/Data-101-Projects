# Recursive Partitioning and Regression Tree RPART library
# Implementation of decision tree
# Resulting Models are represented as a binary tree

# Install and load rpart packages
install.packages("rpart")
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)

# Dataset
# moody<-read.csv('MoodyMarch2022b.csv') # nolint
moody <- read.csv('moody2022_new.csv')

# rpart( formula , method, data, control, subset, weights ...)
# Example 1: (multi-class response)
tree1 <- rpart(GRADE~SCORE, moody)
# node), split, n, loss, yval, (yprob)
# split: split condition
# n: num of observations
# loss: num of missclassified observations
# yval: predicted label

# plot the tree
# each nodes shows the predicted class with probability
# and the percentage of observations in the node
rpart.plot(tree1)

# formula: <prediction> ~ <predictor1 + predictor2 + predictor3 + ... >
# eg. moody$GARDE ~ moody$SCORE + moody$TEXTING_IN_CLASS
# Example 2:
tree2 <- rpart(GRADE~SCORE + TEXTING_IN_CLASS, moody)
rpart.plot(tree2)

# method: types of decision tree we want
# "class": for classification, "anova": for regression
tree3 <- rpart(SCORE~., moody, method='anova')
rpart.plot(tree3)
# each node show the predict value and the percentage of observation in the node

# data: dataset which we want to fit the tree on

# control: control parameters.
# cp: complexity parameter. 
# Any split which does not increase the accuracy of the fit by cp, will not be made.
# eg: cp = 0.001 smaller value means deeper tree
tree4 <- rpart(GRADE~SCORE + TEXTING_IN_CLASS, moody, cp=0.005)
rpart.plot(tree4)
# deviance: the deviance associated with that branch
# predict as 53, how much the observation is deviated from the prediction

printcp(tree4)
# one row per level of the tree
# cp value, nsplit, relative error, xerror, xstd
# cp value = current layer relative error - next layer relative error
# relative error = relative error on data. root node error * rel error
# x-error is the cross-validation error, 
#  generated by the rpart built-in cross validation.
# xstd: standard deviation of the xerror

printcp(tree2) # cp = 0.01

# minsplit: control the number of observations that must exist in a node
# eg. set minsplit = 500, 
tree5 <- rpart(GRADE~SCORE + TEXTING_IN_CLASS, moody, minsplit=300)
rpart.plot(tree5)
nrow(moody[moody$SCORE>=30 & moody$SCORE<80,]) # keep splitting
nrow(moody[moody$SCORE>=51 & moody$SCORE<80 ,]) # stop splitting

# minbucket: minimum number of observations in any terminal(leaf) node.
tree6 <- rpart(GRADE~SCORE + TEXTING_IN_CLASS, moody,minbucket=50)
rpart.plot(tree6)

tree7 <- rpart(GRADE~SCORE + TEXTING_IN_CLASS, moody,minbucket=100)
rpart.plot(tree7)

# predict using decision tree
pred <- predict(tree2, moody, type='class')
# decision tree, data, type

# split your dataset
set.seed(1) # for reproducible
# training dataset
split.per = 0.8
sample <- sample(c(TRUE, FALSE), nrow(moody), replace=TRUE, prob=c(split.per,1-split.per))
moody.train <- moody[sample,]
# testing dataset
moody.test <- moody[!sample,]

tree8 <- rpart(GRADE~., moody.train)
rpart.plot(tree8)
predict.test <- predict(tree8, moody.test, type='class')
printcp(tree8)

# check the test accuracy
mean(moody.test$GRADE==predict.test)
# more closer to xerror
